{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c8ba739",
   "metadata": {},
   "source": [
    "# Part 1. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e0cabd",
   "metadata": {},
   "source": [
    "## **I. Excel Sheet Extraction and CSV Conversion for PySpark Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd54fad5",
   "metadata": {},
   "source": [
    "##### The original raw data file is an Excel workbook (`.xlsx`) containing three separate sheets: `VNI`, `XAUUSD`, and `BTCUSD`. Since PySpark does not natively support reading Excel files, especially those with multiple sheets, it is necessary to first convert each sheet into its own CSV file. This script performs that conversion by reading each sheet and exporting it as an individual CSV. Once the data is in CSV format, each file can then be independently loaded into PySpark for preprocessing tasks such as cleaning, transformation, and analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd2468e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Sheet 'VNI' has been saved as: VNI.csv\n",
      "✅ Sheet 'XAUUSD' has been saved as: XAUUSD.csv\n",
      "✅ Sheet 'BTCUSD' has been saved as: BTCUSD.csv\n",
      "🎉 All sheets have been successfully exported to CSV files!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import openpyxl \n",
    "\n",
    "# Path to your Excel file\n",
    "excel_file = 'C:/Users/ADMIN88/Documents/Big data analysis/Final project/price_raw.xlsx'\n",
    "\n",
    "# Read all specified sheets into a dictionary (using openpyxl since it's an .xlsx file)\n",
    "all_sheets = pd.read_excel(excel_file, sheet_name=['VNI', 'XAUUSD', 'BTCUSD'], engine='openpyxl')\n",
    "\n",
    "# Loop through each sheet and save it as a separate CSV file\n",
    "for sheet_name, df in all_sheets.items():\n",
    "    csv_file = f\"{sheet_name}.csv\"\n",
    "    df.to_csv(csv_file, index=False, encoding='utf-8-sig')\n",
    "    print(f\"✅ Sheet '{sheet_name}' has been saved as: {csv_file}\")\n",
    "\n",
    "print(\"🎉 All sheets have been successfully exported to CSV files!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56757772",
   "metadata": {},
   "source": [
    "## **II. BTCUSD Data Cleaning and Preprocessing with PySpark**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236dcbc9",
   "metadata": {},
   "source": [
    "### 1. Initialize SparkSession for BTC Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4d31775",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import regexp_replace, when, col\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"BTC Data Cleaning\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352082e3",
   "metadata": {},
   "source": [
    "#### Import Libraries and Initialize SparkSession for BTC Data Cleaning\n",
    "\n",
    "This section sets up the PySpark environment needed to perform data preprocessing tasks on the BTC (Bitcoin) historical price data. \n",
    "\n",
    "1. **Imports required PySpark libraries:**\n",
    "   - `SparkSession`: Main entry point for working with DataFrames and the Spark SQL engine.\n",
    "   - `regexp_replace`, `when`, `col`: Common PySpark functions for cleaning and transforming data.\n",
    "   - `DoubleType`: Used to explicitly cast columns to `double` type.\n",
    "\n",
    "2. **Initializes a Spark session:**\n",
    "   - Names the application `\"BTC Data Cleaning\"` for tracking in the Spark UI.\n",
    "   - Uses `.getOrCreate()` to avoid creating a new session if one already exists."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be8c51c",
   "metadata": {},
   "source": [
    "### 2. Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25800259",
   "metadata": {},
   "source": [
    "#### Load BTCUSD CSV File into a PySpark DataFrame\n",
    "\n",
    "In this step, we load the raw historical BTCUSD data from a CSV file into a PySpark DataFrame for further processing.\n",
    "\n",
    "1. **Read the CSV file into a DataFrame:**\n",
    "   - Uses `spark.read.csv()` to read the file located at the specified path.\n",
    "   - `header=True`: Treats the first row of the file as column headers.\n",
    "   - `inferSchema=True`: Automatically infers data types for each column.\n",
    "\n",
    "2. **Preview the DataFrame:**\n",
    "   - `.show(10)`: Displays the first 10 rows of the DataFrame to give a quick look at the data structure and content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "624ff6ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+--------+--------+--------+-------+--------+\n",
      "|      Date|   Price|    Open|    High|     Low|   Vol.|Change %|\n",
      "+----------+--------+--------+--------+--------+-------+--------+\n",
      "|2025-05-10|103511.7|102974.7|104072.0|102837.2| 55.57K|  0.0052|\n",
      "|2025-05-09|102975.1|103270.7|104308.7|102365.7| 87.34K| -0.0029|\n",
      "|2025-05-08|103274.3| 97035.1|103885.4| 96901.9|110.60K|  0.0643|\n",
      "|2025-05-07| 97035.1| 96817.9| 97647.5| 95798.4| 59.98K|  0.0022|\n",
      "|2025-05-06| 96825.4| 94745.9| 96893.9| 93408.8| 55.76K|   0.022|\n",
      "|2025-05-05| 94745.2| 94314.6| 95206.8| 93590.8| 59.21K|  0.0045|\n",
      "|2025-05-04| 94316.9| 95885.5| 96297.8| 94206.1| 41.31K| -0.0164|\n",
      "|2025-05-03| 95885.8| 96894.3| 96922.2| 95785.5| 36.24K| -0.0104|\n",
      "|2025-05-02| 96894.4| 96497.3| 97881.8| 96359.3| 51.94K|  0.0041|\n",
      "|2025-05-01| 96499.3| 94181.3| 97394.3| 94168.1| 74.56K|  0.0246|\n",
      "+----------+--------+--------+--------+--------+-------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read CSV file\n",
    "btc_df = spark.read.csv(\"C:/Users/ADMIN88/Documents/Big data analysis/Final project/BTCUSD.csv\", header=True, inferSchema=True)\n",
    "# Show the first few rows\n",
    "btc_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74713936",
   "metadata": {},
   "source": [
    "#### 📊 Dataset Preview: BTCUSD Historical Price Data\n",
    "\n",
    "This dataset contains historical daily trading data for Bitcoin (BTC) priced in USD, ranging from 2015 to 2025. It includes price movements, trading volume, and daily percentage changes. The data appears to be structured and time-series oriented, suitable for financial analysis and machine learning preprocessing.\n",
    "\n",
    "#### 🧾 Column Descriptions:\n",
    "\n",
    "| Column     | Description                                                                 |\n",
    "|------------|-----------------------------------------------------------------------------|\n",
    "| `Date`     | The calendar date of the BTC trading session (format: YYYY-MM-DD)          |\n",
    "| `Price`    | The closing price of Bitcoin on that date (in USD)                         |\n",
    "| `Open`     | The opening price of Bitcoin for that date                                  |\n",
    "| `High`     | The highest price Bitcoin reached during the trading day                    |\n",
    "| `Low`      | The lowest price Bitcoin reached during the trading day                     |\n",
    "| `Vol.`     | The trading volume for that day (in thousands, e.g., 55.57K = 55,570 BTC)   |\n",
    "| `Change %` | The percentage change in closing price from the previous day                |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28ce18a",
   "metadata": {},
   "source": [
    "#### 🔄 Renaming Columns for Cleaner Processing\n",
    "\n",
    "To prepare the dataset for smoother data manipulation and avoid issues caused by special characters, we renamed the following columns:\n",
    "\n",
    "- **`Vol.` → `Vol`** :  The period `.` can lead to syntax issues in Spark SQL expressions or when referencing column names programmatically. Removing it ensures better compatibility.\n",
    "\n",
    "- **`Change %` → `Change`** : The percentage symbol `%` may cause parsing errors or interfere with Spark functions. Renaming it eliminates potential problems during transformation or analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ac6552f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+--------+--------+--------+-------+-------+\n",
      "|      Date|   Price|    Open|    High|     Low|    Vol| Change|\n",
      "+----------+--------+--------+--------+--------+-------+-------+\n",
      "|2025-05-10|103511.7|102974.7|104072.0|102837.2| 55.57K| 0.0052|\n",
      "|2025-05-09|102975.1|103270.7|104308.7|102365.7| 87.34K|-0.0029|\n",
      "|2025-05-08|103274.3| 97035.1|103885.4| 96901.9|110.60K| 0.0643|\n",
      "|2025-05-07| 97035.1| 96817.9| 97647.5| 95798.4| 59.98K| 0.0022|\n",
      "|2025-05-06| 96825.4| 94745.9| 96893.9| 93408.8| 55.76K|  0.022|\n",
      "|2025-05-05| 94745.2| 94314.6| 95206.8| 93590.8| 59.21K| 0.0045|\n",
      "|2025-05-04| 94316.9| 95885.5| 96297.8| 94206.1| 41.31K|-0.0164|\n",
      "|2025-05-03| 95885.8| 96894.3| 96922.2| 95785.5| 36.24K|-0.0104|\n",
      "|2025-05-02| 96894.4| 96497.3| 97881.8| 96359.3| 51.94K| 0.0041|\n",
      "|2025-05-01| 96499.3| 94181.3| 97394.3| 94168.1| 74.56K| 0.0246|\n",
      "+----------+--------+--------+--------+--------+-------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Rename columns\n",
    "btc_df = btc_df.withColumnRenamed(\"Vol.\", \"Vol\") \\\n",
    "               .withColumnRenamed(\"Change %\", \"Change\")\n",
    "\n",
    "# Show the result\n",
    "btc_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd80930",
   "metadata": {},
   "source": [
    "#### 🧹 Cleaning the 'Vol' Column for Numeric Representation\n",
    "\n",
    "The `Vol` column in the dataset contains values with suffixes like 'K', 'M', and 'B', representing thousands, millions, and billions respectively. To perform accurate calculations and analyses, these values need to be converted into their actual numeric form. Here's how we cleaned the column:\n",
    "\n",
    "1. **Handling 'K' (Thousands)**:  \n",
    "   If the `Vol` value ends with 'K', we remove the 'K' and multiply the remaining number by 1,000 to convert it to the actual number.\n",
    "\n",
    "2. **Handling 'M' (Millions)**:  \n",
    "   If the `Vol` value ends with 'M', we remove the 'M' and multiply the remaining number by 1,000,000 to convert it into millions.\n",
    "\n",
    "3. **Handling 'B' (Billions)**:  \n",
    "   If the `Vol` value ends with 'B', we remove the 'B' and multiply the remaining number by 1,000,000,000 to convert it into billions.\n",
    "\n",
    "4. **Other Cases (No Suffix)**:  \n",
    "   If the `Vol` value does not contain any suffix, we simply remove any commas and cast it into a numeric type (`DoubleType`).\n",
    "\n",
    "This cleaning ensures that the `Vol` column is consistently represented as a numeric value, making it easier to perform analysis, comparisons, and calculations. By transforming the 'K', 'M', and 'B' suffixes into their actual values, we ensure that the data is more suitable for numerical processing and analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29a51e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean 'Vol' column\n",
    "btc_df = btc_df.withColumn(\n",
    "    \"Vol_clean\",\n",
    "    when(col(\"Vol\").endswith(\"K\"), regexp_replace(col(\"Vol\"), \"K\", \"\").cast(DoubleType()) * 1_000)\n",
    "    .when(col(\"Vol\").endswith(\"M\"), regexp_replace(col(\"Vol\"), \"M\", \"\").cast(DoubleType()) * 1_000_000)\n",
    "    .when(col(\"Vol\").endswith(\"B\"), regexp_replace(col(\"Vol\"), \"B\", \"\").cast(DoubleType()) * 1_000_000_000)\n",
    "    .otherwise(regexp_replace(col(\"Vol\"), \",\", \"\").cast(DoubleType()))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b4dc1b",
   "metadata": {},
   "source": [
    "#### 🧹 Finalizing the 'Vol' Column Clean-up\n",
    "\n",
    "After transforming the `Vol` column into numeric values and creating a new column `Vol_clean`, we need to finalize the changes. This involves removing the old `Vol` column and renaming the `Vol_clean` column to `Vol` for consistency.\n",
    "\n",
    "1. **Dropping the Old 'Vol' Column**:  \n",
    "   The old `Vol` column, which still contained non-numeric values (e.g., 'K', 'M', 'B'), is dropped to ensure that only the cleaned data is retained.\n",
    "\n",
    "2. **Renaming 'Vol_clean' to 'Vol'**:  \n",
    "   The `Vol_clean` column, which now contains numeric values, is renamed back to `Vol` to maintain the original column name while ensuring the data is in a usable format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73805e1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+--------+--------+--------+-------+--------+\n",
      "|      Date|   Price|    Open|    High|     Low| Change|     Vol|\n",
      "+----------+--------+--------+--------+--------+-------+--------+\n",
      "|2025-05-10|103511.7|102974.7|104072.0|102837.2| 0.0052| 55570.0|\n",
      "|2025-05-09|102975.1|103270.7|104308.7|102365.7|-0.0029| 87340.0|\n",
      "|2025-05-08|103274.3| 97035.1|103885.4| 96901.9| 0.0643|110600.0|\n",
      "|2025-05-07| 97035.1| 96817.9| 97647.5| 95798.4| 0.0022| 59980.0|\n",
      "|2025-05-06| 96825.4| 94745.9| 96893.9| 93408.8|  0.022| 55760.0|\n",
      "|2025-05-05| 94745.2| 94314.6| 95206.8| 93590.8| 0.0045| 59210.0|\n",
      "|2025-05-04| 94316.9| 95885.5| 96297.8| 94206.1|-0.0164| 41310.0|\n",
      "|2025-05-03| 95885.8| 96894.3| 96922.2| 95785.5|-0.0104| 36240.0|\n",
      "|2025-05-02| 96894.4| 96497.3| 97881.8| 96359.3| 0.0041| 51940.0|\n",
      "|2025-05-01| 96499.3| 94181.3| 97394.3| 94168.1| 0.0246| 74560.0|\n",
      "+----------+--------+--------+--------+--------+-------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Drop old 'Vol' colum\n",
    "btc_df = btc_df.drop(\"Vol\")\n",
    "\n",
    "# Rename cleaned columns\n",
    "btc_df = btc_df.withColumnRenamed(\"Vol_clean\", \"Vol\") \n",
    "\n",
    "# ✅ Check: show sample\n",
    "btc_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f0d1fa",
   "metadata": {},
   "source": [
    "#### 🔄 Reordering Columns for Better Structure\n",
    "\n",
    "In the preview of the dataset, we noticed that the `Vol` column currently appears after the `Change` column. For consistency and clarity in data presentation, we decided to reorder the columns so that the `Vol` column appears before the `Change` column.\n",
    "\n",
    "1. **Reordering Columns**:  \n",
    "   We defined the desired column order, specifying that `Vol` should come before `Change`. This ensures that the columns are logically arranged for easier analysis and reporting.\n",
    "\n",
    "2. **Applying the New Order**:  \n",
    "   Using the `select()` method, we applied the new column order to the DataFrame, which repositions `Vol` before `Change`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2eea7efe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+--------+--------+--------+--------+-------+\n",
      "|      Date|   Price|    Open|    High|     Low|     Vol| Change|\n",
      "+----------+--------+--------+--------+--------+--------+-------+\n",
      "|2025-05-10|103511.7|102974.7|104072.0|102837.2| 55570.0| 0.0052|\n",
      "|2025-05-09|102975.1|103270.7|104308.7|102365.7| 87340.0|-0.0029|\n",
      "|2025-05-08|103274.3| 97035.1|103885.4| 96901.9|110600.0| 0.0643|\n",
      "|2025-05-07| 97035.1| 96817.9| 97647.5| 95798.4| 59980.0| 0.0022|\n",
      "|2025-05-06| 96825.4| 94745.9| 96893.9| 93408.8| 55760.0|  0.022|\n",
      "|2025-05-05| 94745.2| 94314.6| 95206.8| 93590.8| 59210.0| 0.0045|\n",
      "|2025-05-04| 94316.9| 95885.5| 96297.8| 94206.1| 41310.0|-0.0164|\n",
      "|2025-05-03| 95885.8| 96894.3| 96922.2| 95785.5| 36240.0|-0.0104|\n",
      "|2025-05-02| 96894.4| 96497.3| 97881.8| 96359.3| 51940.0| 0.0041|\n",
      "|2025-05-01| 96499.3| 94181.3| 97394.3| 94168.1| 74560.0| 0.0246|\n",
      "+----------+--------+--------+--------+--------+--------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reorder columns (move 'Vol' before 'Change')\n",
    "desired_column_order = ['Date', 'Price', 'Open', 'High', 'Low', 'Vol', 'Change']\n",
    "btc_df = btc_df.select(desired_column_order)\n",
    "\n",
    "# Show the reordered DataFrame\n",
    "btc_df.show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aacfc0f7",
   "metadata": {},
   "source": [
    "#### 📋 Checking the Schema of the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3d9a4c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: date (nullable = true)\n",
      " |-- Price: double (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Vol: double (nullable = true)\n",
      " |-- Change: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check the schema\n",
    "btc_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d42cc87",
   "metadata": {},
   "source": [
    "#### ✅ Schema Validation Result\n",
    "\n",
    "After running `printSchema()`, we confirmed that all columns in the DataFrame have been correctly interpreted and cast to appropriate data types:\n",
    "\n",
    "- `Date`: `date` — correctly parsed from string format.\n",
    "- `Price`, `Open`, `High`, `Low`, `Vol`, `Change`: all are `double` — ensuring numerical operations can be performed efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff39874",
   "metadata": {},
   "source": [
    "#### 🔎 Missing value check in Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dda8ac12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----+----+---+---+------+\n",
      "|Date|Price|Open|High|Low|Vol|Change|\n",
      "+----+-----+----+----+---+---+------+\n",
      "|   0|    0|   0|   0|  0|  0|     0|\n",
      "+----+-----+----+----+---+---+------+\n",
      "\n",
      "🔍 Null Value Check:\n",
      "✅ Column 'Date' has no missing values.\n",
      "✅ Column 'Price' has no missing values.\n",
      "✅ Column 'Open' has no missing values.\n",
      "✅ Column 'High' has no missing values.\n",
      "✅ Column 'Low' has no missing values.\n",
      "✅ Column 'Vol' has no missing values.\n",
      "✅ Column 'Change' has no missing values.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "# Count nulls in each column\n",
    "null_counts = btc_df.select([\n",
    "    sum(col(c).isNull().cast(\"int\")).alias(c) for c in btc_df.columns\n",
    "])\n",
    "\n",
    "# Collect the result to the driver\n",
    "null_counts_dict = null_counts.collect()[0].asDict()\n",
    "\n",
    "null_counts.show()\n",
    "\n",
    "# Print detailed results\n",
    "print(\"🔍 Null Value Check:\")\n",
    "for column, null_count in null_counts_dict.items():\n",
    "    if null_count == 0:\n",
    "        print(f\"✅ Column '{column}' has no missing values.\")\n",
    "    else:\n",
    "        print(f\"⚠️ Column '{column}' has {null_count} missing value(s).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f42a8e5",
   "metadata": {},
   "source": [
    "#### 🔎 Duplicate Row Check in Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6eda9748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔎 Duplicate Row Check:\n",
      "✅ No duplicate rows found in the dataset.\n"
     ]
    }
   ],
   "source": [
    "# ✅ Check for duplicate rows\n",
    "total_rows = btc_df.count()\n",
    "distinct_rows = btc_df.distinct().count()\n",
    "duplicate_count = total_rows - distinct_rows\n",
    "\n",
    "# 📝 Print result\n",
    "print(\"🔎 Duplicate Row Check:\")\n",
    "if duplicate_count == 0:\n",
    "    print(\"✅ No duplicate rows found in the dataset.\")\n",
    "else:\n",
    "    print(f\"⚠️ Found {duplicate_count} duplicate row(s) in the dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5308c67b",
   "metadata": {},
   "source": [
    "#### 📋 Statistical results summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9d988cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------------+------------------+------------------+--------------------+--------------------+\n",
      "|summary|            Price|              Open|              High|               Low|                 Vol|              Change|\n",
      "+-------+-----------------+------------------+------------------+------------------+--------------------+--------------------+\n",
      "|  count|             3783|              3783|              3783|              3783|                3783|                3783|\n",
      "|   mean|22607.29299497749|22580.113269891546|23094.781205392585|22040.250489029742| 1.632940622521808E7|0.002199444885011892|\n",
      "| stddev|25494.95324271896|25463.596431494963|25996.768121511122|  24916.4600436307|1.7787544331153402E8| 0.03641571685211734|\n",
      "|    min|            164.9|             164.9|             212.6|             157.3|               260.0|             -0.3918|\n",
      "|    25%|           2900.3|            2883.3|            2977.9|            2807.4|             56940.0|             -0.0123|\n",
      "|    50%|          10184.8|           10178.1|           10421.4|            9882.7|            102100.0|              0.0013|\n",
      "|    75%|          36982.1|           36962.3|           37887.6|           35561.6|            286940.0|              0.0169|\n",
      "|    max|         106157.2|          106157.2|          109228.6|          105350.6|              4.47E9|               0.272|\n",
      "+-------+-----------------+------------------+------------------+------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Detailed summary statistics including percentiles\n",
    "btc_df.summary().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f112d51",
   "metadata": {},
   "source": [
    "#### 📊 Key Inpretations: \n",
    "\n",
    "This summary table covers **3,783 daily records** of Bitcoin (BTC) trading data. It provides insights into BTC's price behavior, volatility, trading volume, and daily returns over a long time horizon—likely from the early days of Bitcoin trading through 2025.\n",
    "\n",
    "- **Price Trends**:\n",
    "  - The **average closing price** was approximately **$22,607.29**, reflecting substantial long-term value growth for BTC.\n",
    "  - Prices ranged from as low as **$164.90** to a staggering **$106,157.20**, highlighting Bitcoin's **explosive price evolution** over the years.\n",
    "  - The **median (50th percentile)** price was **$10,184.80**, while the 75th percentile jumped to **$36,982.10**, showing a **positively skewed distribution** influenced by recent bull markets.\n",
    "\n",
    "- **Volatility**:\n",
    "  - BTC exhibits **very high standard deviation** (~$25,495), reinforcing its reputation for volatility.\n",
    "  - The volatility is consistent across `Open`, `High`, and `Low` prices, confirming that Bitcoin regularly experiences wide intraday price swings.\n",
    "\n",
    "- **Volume**:\n",
    "  - The **average daily trading volume** was **~16.33 million**, but the **maximum recorded volume reached 4.47 billion**, indicating **extreme spikes in market activity**, likely during speculative bubbles or market corrections.\n",
    "  - High **stddev of volume (~178M)** suggests **inconsistent liquidity**, often driven by sentiment and news cycles.\n",
    "\n",
    "- **Daily Change %**:\n",
    "  - The **mean daily return** is **0.22%**, indicating a general upward bias in price action.\n",
    "  - However, the **standard deviation** of daily returns is **3.64%**, showcasing **frequent and large fluctuations** in day-to-day returns.\n",
    "  - Extreme values range from **-39.18%** to **+27.2%**, underscoring Bitcoin's susceptibility to sharp market moves.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9921b300",
   "metadata": {},
   "source": [
    "#### 📅 Sorting by Date for Time Series Consistency\n",
    "When working with time series data such as Bitcoin historical prices, maintaining chronological order is essential. Without explicitly sorting the dataset, the rows may appear in a random or inconsistent order, which can lead to incorrect insights during trend analysis, forecasting, or visualizations. To ensure the dataset follows the correct timeline, we sort the data by the Date column in ascending order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb096ccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ The dataset has been sorted by date in ascending order.\n"
     ]
    }
   ],
   "source": [
    "# Sort by ascending date\n",
    "df_order = btc_df.orderBy(\"Date\")\n",
    "\n",
    "# Print confirmation message\n",
    "print(\"✅ The dataset has been sorted by date in ascending order.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3955ef0",
   "metadata": {},
   "source": [
    "### 3. Export Cleaned BTC Data to CSV and Stop Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "965119bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ The CSV file has been saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Save the result to a CSV file\n",
    "df_order.toPandas().to_csv(\"C:/Users/ADMIN88/Documents/Big data analysis/Final project/BTC_cleaned.csv\", index=False)\n",
    "\n",
    "# Print confirmation message\n",
    "print(\"✅ The CSV file has been saved successfully.\")\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdded47f",
   "metadata": {},
   "source": [
    "#### 💾 Saving the Cleaned Dataset to CSV\n",
    "\n",
    "After completing all necessary data cleaning and preprocessing steps, we export the final DataFrame to a CSV file for further analysis or sharing. The code performs the following:\n",
    "\n",
    "1. **Convert to Pandas DataFrame**:  \n",
    "   We convert the Spark DataFrame `df_order` to a Pandas DataFrame using `.toPandas()` for compatibility with the `.to_csv()` function.\n",
    "\n",
    "2. **Export to CSV**:  \n",
    "   The cleaned dataset is saved to the specified path\n",
    "\n",
    "3. **Terminate Spark Session**:  \n",
    "Finally, we stop the active Spark session with `spark.stop()` to release resources.\n",
    "\n",
    "This step finalizes our data pipeline and prepares the cleaned dataset for downstream tasks such as visualization, modeling, or reporting.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05038991",
   "metadata": {},
   "source": [
    "## **III. XAU Data Cleaning and Preprocessing with PySpark**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72082fe",
   "metadata": {},
   "source": [
    "### 1. Initialize SparkSession for XAU Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "caac478f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import regexp_replace, when, col\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"XAU Data Cleaning\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea5de60",
   "metadata": {},
   "source": [
    "#### Import Libraries and Initialize SparkSession for XAU Data Cleaning\n",
    "\n",
    "This section sets up the PySpark environment needed to perform data preprocessing tasks on the XAU (Gold) historical price data. \n",
    "\n",
    "1. **Imports required PySpark libraries:**\n",
    "   - `SparkSession`: Main entry point for working with DataFrames and the Spark SQL engine.\n",
    "   - `regexp_replace`, `when`, `col`: Common PySpark functions for cleaning and transforming data.\n",
    "   - `DoubleType`: Used to explicitly cast columns to `double` type.\n",
    "\n",
    "2. **Initializes a Spark session:**\n",
    "   - Names the application `\"XAU Data Cleaning\"` for tracking in the Spark UI.\n",
    "   - Uses `.getOrCreate()` to avoid creating a new session if one already exists."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b931396",
   "metadata": {},
   "source": [
    "### 2. Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6868f2",
   "metadata": {},
   "source": [
    "#### Load XAUUSD CSV File into a PySpark DataFrame\n",
    "\n",
    "In this step, we load the raw historical XAUUSD data from a CSV file into a PySpark DataFrame for further processing.\n",
    "\n",
    "1. **Read the CSV file into a DataFrame:**\n",
    "   - Uses `spark.read.csv()` to read the file located at the specified path.\n",
    "   - `header=True`: Treats the first row of the file as column headers.\n",
    "   - `inferSchema=True`: Automatically infers data types for each column.\n",
    "\n",
    "2. **Preview the DataFrame:**\n",
    "   - `.show(10)`: Displays the first 10 rows of the DataFrame to give a quick look at the data structure and content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2e530cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+------+------+------+-------+--------+\n",
      "|      Date| Price|  Open|  High|   Low|   Vol.|Change %|\n",
      "+----------+------+------+------+------+-------+--------+\n",
      "|2025-05-09|3344.0|3310.2|3352.7|3278.9|210.27K|  0.0115|\n",
      "|2025-05-08|3306.0|3373.1|3422.0|3293.3|327.13K| -0.0253|\n",
      "|2025-05-07|3391.9|3448.1|3448.2|3367.0|304.24K|  -0.009|\n",
      "|2025-05-06|3422.8|3345.7|3444.5|3332.1|268.10K|  0.0303|\n",
      "|2025-05-05|3322.3|3247.1|3346.7|3243.1|184.90K|  0.0244|\n",
      "|2025-05-02|3243.3|3247.6|3277.0|3229.5|210.99K|  0.0065|\n",
      "|2025-05-01|3222.2|3299.0|3300.6|3209.4|214.69K| -0.0292|\n",
      "|2025-04-30|3319.1|3324.5|3337.6|3275.6|207.72K| -0.0043|\n",
      "|2025-04-29|3333.6|3354.9|3359.3|3309.2|170.15K|  2.0E-4|\n",
      "|2025-04-28|3333.0|3318.3|3347.7|3266.3|  1.30K|  0.0105|\n",
      "+----------+------+------+------+------+-------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read CSV file\n",
    "xau_df = spark.read.csv(\"C:/Users/ADMIN88/Documents/Big data analysis/Final project/XAUUSD.csv\", header=True, inferSchema=True)\n",
    "# Show the first few rows\n",
    "xau_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a137427d",
   "metadata": {},
   "source": [
    "#### 📊 Dataset Preview: XAUUSD Historical Price Data\n",
    "\n",
    "This dataset contains historical daily trading data for Gold (XAU) priced in USD, ranging from **January 2, 2015** to **May 9, 2025**. It captures daily market activity including price levels, trading volumes, and day-over-day percentage changes. \n",
    "\n",
    "#### 🧾 Column Descriptions:\n",
    "\n",
    "| Column     | Description                                                                 |\n",
    "|------------|-----------------------------------------------------------------------------|\n",
    "| `Date`     | The calendar date of the XAU trading session (format: YYYY-MM-DD)          |\n",
    "| `Price`    | The closing price of Gold on that date (in USD per ounce)                  |\n",
    "| `Open`     | The opening price of Gold on that date                                      |\n",
    "| `High`     | The highest price Gold reached during the trading day                       |\n",
    "| `Low`      | The lowest price Gold reached during the trading day                        |\n",
    "| `Vol.`     | The trading volume for that day (in thousands, e.g., 210.27K = 210,270 oz)  |\n",
    "| `Change %` | The percentage change in closing price compared to the previous day         |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12dbae0a",
   "metadata": {},
   "source": [
    "#### 🔄 Renaming Columns for Cleaner Processing\n",
    "\n",
    "To prepare the dataset for smoother data manipulation and avoid issues caused by special characters, we renamed the following columns:\n",
    "\n",
    "- **`Vol.` → `Vol`** :  The period `.` can lead to syntax issues in Spark SQL expressions or when referencing column names programmatically. Removing it ensures better compatibility.\n",
    "\n",
    "- **`Change %` → `Change`** : The percentage symbol `%` may cause parsing errors or interfere with Spark functions. Renaming it eliminates potential problems during transformation or analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2c2a31c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+------+------+------+-------+-------+\n",
      "|      Date| Price|  Open|  High|   Low|    Vol| Change|\n",
      "+----------+------+------+------+------+-------+-------+\n",
      "|2025-05-09|3344.0|3310.2|3352.7|3278.9|210.27K| 0.0115|\n",
      "|2025-05-08|3306.0|3373.1|3422.0|3293.3|327.13K|-0.0253|\n",
      "|2025-05-07|3391.9|3448.1|3448.2|3367.0|304.24K| -0.009|\n",
      "|2025-05-06|3422.8|3345.7|3444.5|3332.1|268.10K| 0.0303|\n",
      "|2025-05-05|3322.3|3247.1|3346.7|3243.1|184.90K| 0.0244|\n",
      "|2025-05-02|3243.3|3247.6|3277.0|3229.5|210.99K| 0.0065|\n",
      "|2025-05-01|3222.2|3299.0|3300.6|3209.4|214.69K|-0.0292|\n",
      "|2025-04-30|3319.1|3324.5|3337.6|3275.6|207.72K|-0.0043|\n",
      "|2025-04-29|3333.6|3354.9|3359.3|3309.2|170.15K| 2.0E-4|\n",
      "|2025-04-28|3333.0|3318.3|3347.7|3266.3|  1.30K| 0.0105|\n",
      "+----------+------+------+------+------+-------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Rename columns\n",
    "xau_df = xau_df.withColumnRenamed(\"Vol.\", \"Vol\") \\\n",
    "               .withColumnRenamed(\"Change %\", \"Change\")\n",
    "\n",
    "# Show the result\n",
    "xau_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2d4ece",
   "metadata": {},
   "source": [
    "#### 🧹 Cleaning the 'Vol' Column for Numeric Representation\n",
    "\n",
    "The `Vol` column in the dataset contains values with suffixes like 'K', 'M', and 'B', representing thousands, millions, and billions respectively. To perform accurate calculations and analyses, these values need to be converted into their actual numeric form. Here's how we cleaned the column:\n",
    "\n",
    "1. **Handling 'K' (Thousands)**:  \n",
    "   If the `Vol` value ends with 'K', we remove the 'K' and multiply the remaining number by 1,000 to convert it to the actual number.\n",
    "\n",
    "2. **Handling 'M' (Millions)**:  \n",
    "   If the `Vol` value ends with 'M', we remove the 'M' and multiply the remaining number by 1,000,000 to convert it into millions.\n",
    "\n",
    "3. **Handling 'B' (Billions)**:  \n",
    "   If the `Vol` value ends with 'B', we remove the 'B' and multiply the remaining number by 1,000,000,000 to convert it into billions.\n",
    "\n",
    "4. **Other Cases (No Suffix)**:  \n",
    "   If the `Vol` value does not contain any suffix, we simply remove any commas and cast it into a numeric type (`DoubleType`).\n",
    "\n",
    "This cleaning ensures that the `Vol` column is consistently represented as a numeric value, making it easier to perform analysis, comparisons, and calculations. By transforming the 'K', 'M', and 'B' suffixes into their actual values, we ensure that the data is more suitable for numerical processing and analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1415a462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean 'Vol' column\n",
    "xau_df = xau_df.withColumn(\n",
    "    \"Vol_clean\",\n",
    "    when(col(\"Vol\").endswith(\"K\"), regexp_replace(col(\"Vol\"), \"K\", \"\").cast(DoubleType()) * 1_000)\n",
    "    .when(col(\"Vol\").endswith(\"M\"), regexp_replace(col(\"Vol\"), \"M\", \"\").cast(DoubleType()) * 1_000_000)\n",
    "    .when(col(\"Vol\").endswith(\"B\"), regexp_replace(col(\"Vol\"), \"B\", \"\").cast(DoubleType()) * 1_000_000_000)\n",
    "    .otherwise(regexp_replace(col(\"Vol\"), \",\", \"\").cast(DoubleType()))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b241cc0",
   "metadata": {},
   "source": [
    "#### 🧹 Finalizing the 'Vol' Column Clean-up\n",
    "\n",
    "After transforming the `Vol` column into numeric values and creating a new column `Vol_clean`, we need to finalize the changes. This involves removing the old `Vol` column and renaming the `Vol_clean` column to `Vol` for consistency.\n",
    "\n",
    "1. **Dropping the Old 'Vol' Column**:  \n",
    "   The old `Vol` column, which still contained non-numeric values (e.g., 'K', 'M', 'B'), is dropped to ensure that only the cleaned data is retained.\n",
    "\n",
    "2. **Renaming 'Vol_clean' to 'Vol'**:  \n",
    "   The `Vol_clean` column, which now contains numeric values, is renamed back to `Vol` to maintain the original column name while ensuring the data is in a usable format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "83f9b981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+------+------+------+-------+--------+\n",
      "|      Date| Price|  Open|  High|   Low| Change|     Vol|\n",
      "+----------+------+------+------+------+-------+--------+\n",
      "|2025-05-09|3344.0|3310.2|3352.7|3278.9| 0.0115|210270.0|\n",
      "|2025-05-08|3306.0|3373.1|3422.0|3293.3|-0.0253|327130.0|\n",
      "|2025-05-07|3391.9|3448.1|3448.2|3367.0| -0.009|304240.0|\n",
      "|2025-05-06|3422.8|3345.7|3444.5|3332.1| 0.0303|268100.0|\n",
      "|2025-05-05|3322.3|3247.1|3346.7|3243.1| 0.0244|184900.0|\n",
      "|2025-05-02|3243.3|3247.6|3277.0|3229.5| 0.0065|210990.0|\n",
      "|2025-05-01|3222.2|3299.0|3300.6|3209.4|-0.0292|214690.0|\n",
      "|2025-04-30|3319.1|3324.5|3337.6|3275.6|-0.0043|207720.0|\n",
      "|2025-04-29|3333.6|3354.9|3359.3|3309.2| 2.0E-4|170150.0|\n",
      "|2025-04-28|3333.0|3318.3|3347.7|3266.3| 0.0105|  1300.0|\n",
      "+----------+------+------+------+------+-------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Drop old 'Vol' colum\n",
    "xau_df = xau_df.drop(\"Vol\")\n",
    "\n",
    "# Rename cleaned columns\n",
    "xau_df = xau_df.withColumnRenamed(\"Vol_clean\", \"Vol\") \n",
    "\n",
    "# ✅ Check: show sample\n",
    "xau_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac7a28b",
   "metadata": {},
   "source": [
    "#### 🔄 Reordering Columns for Better Structure\n",
    "\n",
    "In the preview of the dataset, we noticed that the `Vol` column currently appears after the `Change` column. For consistency and clarity in data presentation, we decided to reorder the columns so that the `Vol` column appears before the `Change` column.\n",
    "\n",
    "1. **Reordering Columns**:  \n",
    "   We defined the desired column order, specifying that `Vol` should come before `Change`. This ensures that the columns are logically arranged for easier analysis and reporting.\n",
    "\n",
    "2. **Applying the New Order**:  \n",
    "   Using the `select()` method, we applied the new column order to the DataFrame, which repositions `Vol` before `Change`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1de1fddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+------+------+------+--------+-------+\n",
      "|      Date| Price|  Open|  High|   Low|     Vol| Change|\n",
      "+----------+------+------+------+------+--------+-------+\n",
      "|2025-05-09|3344.0|3310.2|3352.7|3278.9|210270.0| 0.0115|\n",
      "|2025-05-08|3306.0|3373.1|3422.0|3293.3|327130.0|-0.0253|\n",
      "|2025-05-07|3391.9|3448.1|3448.2|3367.0|304240.0| -0.009|\n",
      "|2025-05-06|3422.8|3345.7|3444.5|3332.1|268100.0| 0.0303|\n",
      "|2025-05-05|3322.3|3247.1|3346.7|3243.1|184900.0| 0.0244|\n",
      "|2025-05-02|3243.3|3247.6|3277.0|3229.5|210990.0| 0.0065|\n",
      "|2025-05-01|3222.2|3299.0|3300.6|3209.4|214690.0|-0.0292|\n",
      "|2025-04-30|3319.1|3324.5|3337.6|3275.6|207720.0|-0.0043|\n",
      "|2025-04-29|3333.6|3354.9|3359.3|3309.2|170150.0| 2.0E-4|\n",
      "|2025-04-28|3333.0|3318.3|3347.7|3266.3|  1300.0| 0.0105|\n",
      "+----------+------+------+------+------+--------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reorder columns (move 'Vol' before 'Change')\n",
    "desired_column_order = ['Date', 'Price', 'Open', 'High', 'Low', 'Vol', 'Change']\n",
    "xau_df = xau_df.select(desired_column_order)\n",
    "\n",
    "# Show the reordered DataFrame\n",
    "xau_df.show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c4b978",
   "metadata": {},
   "source": [
    "#### 📋 Checking the Schema of the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ed597dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: date (nullable = true)\n",
      " |-- Price: double (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Vol: double (nullable = true)\n",
      " |-- Change: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check the schema\n",
    "xau_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5bbb40",
   "metadata": {},
   "source": [
    "#### ✅ Schema Validation Result\n",
    "\n",
    "After running `printSchema()`, we confirmed that all columns in the DataFrame have been correctly interpreted and cast to appropriate data types:\n",
    "\n",
    "- `Date`: `date` — correctly parsed from string format.\n",
    "- `Price`, `Open`, `High`, `Low`, `Vol`, `Change`: all are `double` — ensuring numerical operations can be performed efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277a21b7",
   "metadata": {},
   "source": [
    "#### 🔎 Missing value check in Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9cd774bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----+----+---+---+------+\n",
      "|Date|Price|Open|High|Low|Vol|Change|\n",
      "+----+-----+----+----+---+---+------+\n",
      "|   0|    0|   0|   0|  0|  2|     0|\n",
      "+----+-----+----+----+---+---+------+\n",
      "\n",
      "🔍 Null Value Check:\n",
      "✅ Column 'Date' has no missing values.\n",
      "✅ Column 'Price' has no missing values.\n",
      "✅ Column 'Open' has no missing values.\n",
      "✅ Column 'High' has no missing values.\n",
      "✅ Column 'Low' has no missing values.\n",
      "⚠️ Column 'Vol' has 2 missing value(s).\n",
      "✅ Column 'Change' has no missing values.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "# Count nulls in each column\n",
    "null_counts = xau_df.select([\n",
    "    sum(col(c).isNull().cast(\"int\")).alias(c) for c in xau_df.columns\n",
    "])\n",
    "\n",
    "# Collect the result to the driver\n",
    "null_counts_dict = null_counts.collect()[0].asDict()\n",
    "\n",
    "null_counts.show()\n",
    "\n",
    "# Print detailed results\n",
    "print(\"🔍 Null Value Check:\")\n",
    "for column, null_count in null_counts_dict.items():\n",
    "    if null_count == 0:\n",
    "        print(f\"✅ Column '{column}' has no missing values.\")\n",
    "    else:\n",
    "        print(f\"⚠️ Column '{column}' has {null_count} missing value(s).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78d1f1b",
   "metadata": {},
   "source": [
    "#### 📋 Handling with missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "74906bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Missing values in 'Vol' column have been filled with the mean value.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary functions\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Calculate mean of the 'Vol' column\n",
    "mean_vol = xau_df.select(F.mean(\"Vol\")).collect()[0][0]\n",
    "\n",
    "# Fill missing values in the 'Vol' column with the mean\n",
    "xau_df = xau_df.fillna({'Vol': mean_vol})\n",
    "\n",
    "# Print confirmation message\n",
    "print(f\"✅ Missing values in 'Vol' column have been filled with the mean value.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5683f4",
   "metadata": {},
   "source": [
    "#### ✅ Recheck if there are any remaining missing values in \"Vol\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "07b9e544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No missing values left in the 'Vol' column.\n"
     ]
    }
   ],
   "source": [
    "# Recheck if there are any remaining missing values in \"Vol\"\n",
    "missing_count = xau_df.filter(F.col(\"Vol\").isNull()).count()\n",
    "\n",
    "if missing_count == 0:\n",
    "    print(\"No missing values left in the 'Vol' column.\")\n",
    "else:\n",
    "    print(f\"There are {missing_count} missing values in the 'Vol' column.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cddc541",
   "metadata": {},
   "source": [
    "#### 🔎 Duplicate Row Check in Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8ef86856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔎 Duplicate Row Check:\n",
      "✅ No duplicate rows found in the dataset.\n"
     ]
    }
   ],
   "source": [
    "# ✅ Check for duplicate rows\n",
    "total_rows = xau_df.count()\n",
    "distinct_rows = xau_df.distinct().count()\n",
    "duplicate_count = total_rows - distinct_rows\n",
    "\n",
    "# 📝 Print result\n",
    "print(\"🔎 Duplicate Row Check:\")\n",
    "if duplicate_count == 0:\n",
    "    print(\"✅ No duplicate rows found in the dataset.\")\n",
    "else:\n",
    "    print(f\"⚠️ Found {duplicate_count} duplicate row(s) in the dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81126b1a",
   "metadata": {},
   "source": [
    "#### 📋 Statistical results summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8a9744c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+------------------+------------------+--------------------+\n",
      "|summary|             Price|              Open|              High|               Low|               Vol|              Change|\n",
      "+-------+------------------+------------------+------------------+------------------+------------------+--------------------+\n",
      "|  count|              2653|              2653|              2653|              2653|              2653|                2653|\n",
      "|   mean|1653.6124387485856|1653.5597625329815|1665.0171880889557| 1642.046683000381|129573.40626178801|4.353938937052375...|\n",
      "| stddev|462.44189424631185| 462.0938485287363|466.74527542432367|457.76725241069994|130175.02274962829|0.009356735632787645|\n",
      "|    min|            1049.6|            1051.5|            1062.7|            1045.4|             120.0|             -0.0499|\n",
      "|    25%|            1269.3|            1268.5|            1275.9|            1262.4|            1150.0|             -0.0041|\n",
      "|    50%|            1629.0|            1627.0|            1646.4|            1611.0|          133240.0|              3.0E-4|\n",
      "|    75%|            1901.5|            1901.2|            1913.4|            1887.7|          220930.0|              0.0056|\n",
      "|    max|            3425.3|            3448.1|            3509.9|            3379.1|          816530.0|              0.0595|\n",
      "+-------+------------------+------------------+------------------+------------------+------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Detailed summary statistics including percentiles\n",
    "xau_df.summary().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723e84fc",
   "metadata": {},
   "source": [
    "#### 📈 Key Inpretations:\n",
    "\n",
    "This summary table offers a statistical overview of 2,653 daily records for Gold (XAU) traded in USD from 2015 to 2025. The data includes key indicators such as daily price points, trading volume, and percentage changes. Here's a breakdown of the most notable observations:\n",
    "\n",
    "- **Price and Volatility**:\n",
    "  - The average closing price is **$1,653.61**, with a standard deviation of **$462.44**, indicating a moderate level of price volatility over the observed period.\n",
    "  - Prices ranged from a **minimum of $1,049.60** to a **maximum of $3,425.30**, capturing significant long-term growth and fluctuations in the gold market.\n",
    "\n",
    "- **Trading Volume**:\n",
    "  - The average volume is around **129,573 ounces per day**, with a high standard deviation (**130,175**), reflecting substantial variation in daily trading activity.\n",
    "  - Volume ranged from **just 120 oz** to a peak of **816,530 oz**, suggesting some days had exceptionally high market participation.\n",
    "\n",
    "- **Price Movement Range**:\n",
    "  - The median values (50%) for `Open`, `High`, `Low`, and `Price` all hover around **$1,627–$1,646**, aligning with the overall mean and indicating a relatively symmetric distribution of central prices.\n",
    "\n",
    "- **Daily Change (%)**:\n",
    "  - The mean daily percentage change is approximately **0.00435**, or **0.435%**, with a standard deviation of **0.0094**, reflecting mild daily price fluctuations.\n",
    "  - The most extreme recorded daily percentage change is **+5.95%** and **-4.99%**, suggesting rare but impactful volatility events."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67eb70cf",
   "metadata": {},
   "source": [
    "#### 📅 Sorting by Date for Time Series Consistency\n",
    "When working with time series data such as Gold historical prices, maintaining chronological order is essential. Without explicitly sorting the dataset, the rows may appear in a random or inconsistent order, which can lead to incorrect insights during trend analysis, forecasting, or visualizations. To ensure the dataset follows the correct timeline, we sort the data by the Date column in ascending order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e14de831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ The dataset has been sorted by date in ascending order.\n"
     ]
    }
   ],
   "source": [
    "# Sort by ascending date\n",
    "df_order = xau_df.orderBy(\"Date\")\n",
    "\n",
    "# Print confirmation message\n",
    "print(\"✅ The dataset has been sorted by date in ascending order.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f729a2",
   "metadata": {},
   "source": [
    "### 3. Export Cleaned XAU Data to CSV and Stop Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d0424de0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ The CSV file has been saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Save the result to a CSV file\n",
    "df_order.toPandas().to_csv(\"C:/Users/ADMIN88/Documents/Big data analysis/Final project/XAU_cleaned.csv\", index=False)\n",
    "\n",
    "# Print confirmation message\n",
    "print(\"✅ The CSV file has been saved successfully.\")\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e66f16e",
   "metadata": {},
   "source": [
    "#### 💾 Saving the Cleaned Dataset to CSV\n",
    "\n",
    "After completing all necessary data cleaning and preprocessing steps, we export the final DataFrame to a CSV file for further analysis or sharing. The code performs the following:\n",
    "\n",
    "1. **Convert to Pandas DataFrame**:  \n",
    "   We convert the Spark DataFrame `df_order` to a Pandas DataFrame using `.toPandas()` for compatibility with the `.to_csv()` function.\n",
    "\n",
    "2. **Export to CSV**:  \n",
    "   The cleaned dataset is saved to the specified path\n",
    "\n",
    "3. **Terminate Spark Session**:  \n",
    "Finally, we stop the active Spark session with `spark.stop()` to release resources.\n",
    "\n",
    "This step finalizes our data pipeline and prepares the cleaned dataset for downstream tasks such as visualization, modeling, or reporting.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8269575d",
   "metadata": {},
   "source": [
    "## **IV. VNI Data Cleaning and Preprocessing with PySpark**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de69ec4c",
   "metadata": {},
   "source": [
    "### 1. Initialize SparkSession for VNI Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fcca8af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import regexp_replace, when, col\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"VNI Data Cleaning\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ac0f2e",
   "metadata": {},
   "source": [
    "#### Import Libraries and Initialize SparkSession for VNI Data Cleaning\n",
    "\n",
    "This section sets up the PySpark environment needed to perform data preprocessing tasks on the VNI historical data. \n",
    "\n",
    "1. **Imports required PySpark libraries:**\n",
    "   - `SparkSession`: Main entry point for working with DataFrames and the Spark SQL engine.\n",
    "   - `regexp_replace`, `when`, `col`: Common PySpark functions for cleaning and transforming data.\n",
    "   - `DoubleType`: Used to explicitly cast columns to `double` type.\n",
    "\n",
    "2. **Initializes a Spark session:**\n",
    "   - Names the application `\"VNI Data Cleaning\"` for tracking in the Spark UI.\n",
    "   - Uses `.getOrCreate()` to avoid creating a new session if one already exists."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f414c792",
   "metadata": {},
   "source": [
    "### 2. Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51999f6",
   "metadata": {},
   "source": [
    "#### Load VNI CSV File into a PySpark DataFrame\n",
    "\n",
    "In this step, we load the raw historical VNI data from a CSV file into a PySpark DataFrame for further processing.\n",
    "\n",
    "1. **Read the CSV file into a DataFrame:**\n",
    "   - Uses `spark.read.csv()` to read the file located at the specified path.\n",
    "   - `header=True`: Treats the first row of the file as column headers.\n",
    "   - `inferSchema=True`: Automatically infers data types for each column.\n",
    "\n",
    "2. **Preview the DataFrame:**\n",
    "   - `.show(10)`: Displays the first 10 rows of the DataFrame to give a quick look at the data structure and content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6905e58b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+-------+-------+-------+-------+--------+\n",
      "|      Date|  Price|   Open|   High|    Low|   Vol.|Change %|\n",
      "+----------+-------+-------+-------+-------+-------+--------+\n",
      "|2025-05-09| 1267.3| 1269.8|1279.61|1264.87|693.91M|  -0.002|\n",
      "|2025-05-08| 1269.8|1250.37|1271.44|1250.37|780.07M|  0.0155|\n",
      "|2025-05-07|1250.37|1241.95|1250.79| 1240.2|696.10M|  0.0068|\n",
      "|2025-05-06|1241.95|1240.05|1251.02|1240.05|723.04M|  0.0015|\n",
      "|2025-05-05|1240.05| 1226.3|1241.51| 1226.3|562.29M|  0.0112|\n",
      "|2025-04-29| 1226.3| 1226.8| 1229.1| 1222.3|675.96M| -4.0E-4|\n",
      "|2025-04-28| 1226.8| 1234.3| 1234.3|1222.56|679.79M|  -0.002|\n",
      "|2025-04-25|1229.23|1223.35|1230.72|1220.67|863.02M|  0.0048|\n",
      "|2025-04-24|1223.35|1214.78|1224.66|1210.37|798.64M|  0.0102|\n",
      "|2025-04-23| 1211.0|1197.13|1216.28|1197.13|855.21M|  0.0116|\n",
      "+----------+-------+-------+-------+-------+-------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read CSV file\n",
    "vni_df = spark.read.csv(\"C:/Users/ADMIN88/Documents/Big data analysis/Final project/VNI.csv\", header=True, inferSchema=True)\n",
    "# Show the first few rows\n",
    "vni_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb030cd",
   "metadata": {},
   "source": [
    "#### 📊 Dataset Preview: VNI Index Historical Price Data\n",
    "\n",
    "This dataset contains historical daily trading data for the **VN-Index (VNI)** — the benchmark stock index of Vietnam — spanning from **May 1, 2015, to May 9, 2025**. It captures essential market information such as opening and closing prices, intraday highs and lows, trading volume, and daily percentage changes. \n",
    "\n",
    "#### 🧾 Column Descriptions:\n",
    "\n",
    "| Column     | Description                                                                 |\n",
    "|------------|-----------------------------------------------------------------------------|\n",
    "| `Date`     | The calendar date of the VNI trading session (format: YYYY-MM-DD)          |\n",
    "| `Price`    | The closing value of the VN-Index on that date                              |\n",
    "| `Open`     | The opening value of the VN-Index for that trading day                     |\n",
    "| `High`     | The highest index value recorded during the trading session                |\n",
    "| `Low`      | The lowest index value recorded during the trading session                 |\n",
    "| `Vol.`     | The total trading volume on that day  |\n",
    "| `Change %` | The daily percentage change in closing value compared to the previous day  |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef6bb77",
   "metadata": {},
   "source": [
    "#### 🔄 Renaming Columns for Cleaner Processing\n",
    "\n",
    "To prepare the dataset for smoother data manipulation and avoid issues caused by special characters, we renamed the following columns:\n",
    "\n",
    "- **`Vol.` → `Vol`** :  The period `.` can lead to syntax issues in Spark SQL expressions or when referencing column names programmatically. Removing it ensures better compatibility.\n",
    "\n",
    "- **`Change %` → `Change`** : The percentage symbol `%` may cause parsing errors or interfere with Spark functions. Renaming it eliminates potential problems during transformation or analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "45dbe041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+-------+-------+-------+-------+-------+\n",
      "|      Date|  Price|   Open|   High|    Low|    Vol| Change|\n",
      "+----------+-------+-------+-------+-------+-------+-------+\n",
      "|2025-05-09| 1267.3| 1269.8|1279.61|1264.87|693.91M| -0.002|\n",
      "|2025-05-08| 1269.8|1250.37|1271.44|1250.37|780.07M| 0.0155|\n",
      "|2025-05-07|1250.37|1241.95|1250.79| 1240.2|696.10M| 0.0068|\n",
      "|2025-05-06|1241.95|1240.05|1251.02|1240.05|723.04M| 0.0015|\n",
      "|2025-05-05|1240.05| 1226.3|1241.51| 1226.3|562.29M| 0.0112|\n",
      "|2025-04-29| 1226.3| 1226.8| 1229.1| 1222.3|675.96M|-4.0E-4|\n",
      "|2025-04-28| 1226.8| 1234.3| 1234.3|1222.56|679.79M| -0.002|\n",
      "|2025-04-25|1229.23|1223.35|1230.72|1220.67|863.02M| 0.0048|\n",
      "|2025-04-24|1223.35|1214.78|1224.66|1210.37|798.64M| 0.0102|\n",
      "|2025-04-23| 1211.0|1197.13|1216.28|1197.13|855.21M| 0.0116|\n",
      "+----------+-------+-------+-------+-------+-------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Rename columns\n",
    "vni_df = vni_df.withColumnRenamed(\"Vol.\", \"Vol\") \\\n",
    "               .withColumnRenamed(\"Change %\", \"Change\")\n",
    "\n",
    "# Show the result\n",
    "vni_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1eed32b",
   "metadata": {},
   "source": [
    "#### 🧹 Cleaning the 'Vol' Column for Numeric Representation\n",
    "\n",
    "The `Vol` column in the dataset contains values with suffixes like 'K', 'M', and 'B', representing thousands, millions, and billions respectively. To perform accurate calculations and analyses, these values need to be converted into their actual numeric form. Here's how we cleaned the column:\n",
    "\n",
    "1. **Handling 'K' (Thousands)**:  \n",
    "   If the `Vol` value ends with 'K', we remove the 'K' and multiply the remaining number by 1,000 to convert it to the actual number.\n",
    "\n",
    "2. **Handling 'M' (Millions)**:  \n",
    "   If the `Vol` value ends with 'M', we remove the 'M' and multiply the remaining number by 1,000,000 to convert it into millions.\n",
    "\n",
    "3. **Handling 'B' (Billions)**:  \n",
    "   If the `Vol` value ends with 'B', we remove the 'B' and multiply the remaining number by 1,000,000,000 to convert it into billions.\n",
    "\n",
    "4. **Other Cases (No Suffix)**:  \n",
    "   If the `Vol` value does not contain any suffix, we simply remove any commas and cast it into a numeric type (`DoubleType`).\n",
    "\n",
    "This cleaning ensures that the `Vol` column is consistently represented as a numeric value, making it easier to perform analysis, comparisons, and calculations. By transforming the 'K', 'M', and 'B' suffixes into their actual values, we ensure that the data is more suitable for numerical processing and analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a8e3e7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean 'Vol' column\n",
    "vni_df = vni_df.withColumn(\n",
    "    \"Vol_clean\",\n",
    "    when(col(\"Vol\").endswith(\"K\"), regexp_replace(col(\"Vol\"), \"K\", \"\").cast(DoubleType()) * 1_000)\n",
    "    .when(col(\"Vol\").endswith(\"M\"), regexp_replace(col(\"Vol\"), \"M\", \"\").cast(DoubleType()) * 1_000_000)\n",
    "    .when(col(\"Vol\").endswith(\"B\"), regexp_replace(col(\"Vol\"), \"B\", \"\").cast(DoubleType()) * 1_000_000_000)\n",
    "    .otherwise(regexp_replace(col(\"Vol\"), \",\", \"\").cast(DoubleType()))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120f92d9",
   "metadata": {},
   "source": [
    "#### 🧹 Finalizing the 'Vol' Column Clean-up\n",
    "\n",
    "After transforming the `Vol` column into numeric values and creating a new column `Vol_clean`, we need to finalize the changes. This involves removing the old `Vol` column and renaming the `Vol_clean` column to `Vol` for consistency.\n",
    "\n",
    "1. **Dropping the Old 'Vol' Column**:  \n",
    "   The old `Vol` column, which still contained non-numeric values (e.g., 'K', 'M', 'B'), is dropped to ensure that only the cleaned data is retained.\n",
    "\n",
    "2. **Renaming 'Vol_clean' to 'Vol'**:  \n",
    "   The `Vol_clean` column, which now contains numeric values, is renamed back to `Vol` to maintain the original column name while ensuring the data is in a usable format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8b742640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+-------+-------+-------+-------+--------+\n",
      "|      Date|  Price|   Open|   High|    Low| Change|     Vol|\n",
      "+----------+-------+-------+-------+-------+-------+--------+\n",
      "|2025-05-09| 1267.3| 1269.8|1279.61|1264.87| -0.002|6.9391E8|\n",
      "|2025-05-08| 1269.8|1250.37|1271.44|1250.37| 0.0155|7.8007E8|\n",
      "|2025-05-07|1250.37|1241.95|1250.79| 1240.2| 0.0068| 6.961E8|\n",
      "|2025-05-06|1241.95|1240.05|1251.02|1240.05| 0.0015|7.2304E8|\n",
      "|2025-05-05|1240.05| 1226.3|1241.51| 1226.3| 0.0112|5.6229E8|\n",
      "|2025-04-29| 1226.3| 1226.8| 1229.1| 1222.3|-4.0E-4|6.7596E8|\n",
      "|2025-04-28| 1226.8| 1234.3| 1234.3|1222.56| -0.002|6.7979E8|\n",
      "|2025-04-25|1229.23|1223.35|1230.72|1220.67| 0.0048|8.6302E8|\n",
      "|2025-04-24|1223.35|1214.78|1224.66|1210.37| 0.0102|7.9864E8|\n",
      "|2025-04-23| 1211.0|1197.13|1216.28|1197.13| 0.0116|8.5521E8|\n",
      "+----------+-------+-------+-------+-------+-------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Drop old 'Vol' colum\n",
    "vni_df = vni_df.drop(\"Vol\")\n",
    "\n",
    "# Rename cleaned columns\n",
    "vni_df = vni_df.withColumnRenamed(\"Vol_clean\", \"Vol\") \n",
    "\n",
    "# ✅ Check: show sample\n",
    "vni_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3923b7",
   "metadata": {},
   "source": [
    "#### 🔄 Reordering Columns for Better Structure\n",
    "\n",
    "In the preview of the dataset, we noticed that the `Vol` column currently appears after the `Change` column. For consistency and clarity in data presentation, we decided to reorder the columns so that the `Vol` column appears before the `Change` column.\n",
    "\n",
    "1. **Reordering Columns**:  \n",
    "   We defined the desired column order, specifying that `Vol` should come before `Change`. This ensures that the columns are logically arranged for easier analysis and reporting.\n",
    "\n",
    "2. **Applying the New Order**:  \n",
    "   Using the `select()` method, we applied the new column order to the DataFrame, which repositions `Vol` before `Change`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c2c82678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+-------+-------+-------+--------+-------+\n",
      "|      Date|  Price|   Open|   High|    Low|     Vol| Change|\n",
      "+----------+-------+-------+-------+-------+--------+-------+\n",
      "|2025-05-09| 1267.3| 1269.8|1279.61|1264.87|6.9391E8| -0.002|\n",
      "|2025-05-08| 1269.8|1250.37|1271.44|1250.37|7.8007E8| 0.0155|\n",
      "|2025-05-07|1250.37|1241.95|1250.79| 1240.2| 6.961E8| 0.0068|\n",
      "|2025-05-06|1241.95|1240.05|1251.02|1240.05|7.2304E8| 0.0015|\n",
      "|2025-05-05|1240.05| 1226.3|1241.51| 1226.3|5.6229E8| 0.0112|\n",
      "|2025-04-29| 1226.3| 1226.8| 1229.1| 1222.3|6.7596E8|-4.0E-4|\n",
      "|2025-04-28| 1226.8| 1234.3| 1234.3|1222.56|6.7979E8| -0.002|\n",
      "|2025-04-25|1229.23|1223.35|1230.72|1220.67|8.6302E8| 0.0048|\n",
      "|2025-04-24|1223.35|1214.78|1224.66|1210.37|7.9864E8| 0.0102|\n",
      "|2025-04-23| 1211.0|1197.13|1216.28|1197.13|8.5521E8| 0.0116|\n",
      "+----------+-------+-------+-------+-------+--------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reorder columns (move 'Vol' before 'Change')\n",
    "desired_column_order = ['Date', 'Price', 'Open', 'High', 'Low', 'Vol', 'Change']\n",
    "vni_df = vni_df.select(desired_column_order)\n",
    "\n",
    "# Show the reordered DataFrame\n",
    "vni_df.show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd39d4c8",
   "metadata": {},
   "source": [
    "#### 📋 Checking the Schema of the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "45cac095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: date (nullable = true)\n",
      " |-- Price: double (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Vol: double (nullable = true)\n",
      " |-- Change: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check the schema\n",
    "vni_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279b16e9",
   "metadata": {},
   "source": [
    "#### ✅ Schema Validation Result\n",
    "\n",
    "After running `printSchema()`, we confirmed that all columns in the DataFrame have been correctly interpreted and cast to appropriate data types:\n",
    "\n",
    "- `Date`: `date` — correctly parsed from string format.\n",
    "- `Price`, `Open`, `High`, `Low`, `Vol`, `Change`: all are `double` — ensuring numerical operations can be performed efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30159091",
   "metadata": {},
   "source": [
    "#### 🔎 Missing value check in Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c172785c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----+----+---+---+------+\n",
      "|Date|Price|Open|High|Low|Vol|Change|\n",
      "+----+-----+----+----+---+---+------+\n",
      "|   0|    0|   0|   0|  0|  0|     0|\n",
      "+----+-----+----+----+---+---+------+\n",
      "\n",
      "🔍 Null Value Check:\n",
      "✅ Column 'Date' has no missing values.\n",
      "✅ Column 'Price' has no missing values.\n",
      "✅ Column 'Open' has no missing values.\n",
      "✅ Column 'High' has no missing values.\n",
      "✅ Column 'Low' has no missing values.\n",
      "✅ Column 'Vol' has no missing values.\n",
      "✅ Column 'Change' has no missing values.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "# Count nulls in each column\n",
    "null_counts = vni_df.select([\n",
    "    sum(col(c).isNull().cast(\"int\")).alias(c) for c in vni_df.columns\n",
    "])\n",
    "\n",
    "# Collect the result to the driver\n",
    "null_counts_dict = null_counts.collect()[0].asDict()\n",
    "\n",
    "null_counts.show()\n",
    "\n",
    "# Print detailed results\n",
    "print(\"🔍 Null Value Check:\")\n",
    "for column, null_count in null_counts_dict.items():\n",
    "    if null_count == 0:\n",
    "        print(f\"✅ Column '{column}' has no missing values.\")\n",
    "    else:\n",
    "        print(f\"⚠️ Column '{column}' has {null_count} missing value(s).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344f6898",
   "metadata": {},
   "source": [
    "#### 🔎 Duplicate Row Check in Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4cb787e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔎 Duplicate Row Check:\n",
      "✅ No duplicate rows found in the dataset.\n"
     ]
    }
   ],
   "source": [
    "# ✅ Check for duplicate rows\n",
    "total_rows = vni_df.count()\n",
    "distinct_rows = vni_df.distinct().count()\n",
    "duplicate_count = total_rows - distinct_rows\n",
    "\n",
    "# 📝 Print result\n",
    "print(\"🔎 Duplicate Row Check:\")\n",
    "if duplicate_count == 0:\n",
    "    print(\"✅ No duplicate rows found in the dataset.\")\n",
    "else:\n",
    "    print(f\"⚠️ Found {duplicate_count} duplicate row(s) in the dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a7d76b",
   "metadata": {},
   "source": [
    "#### 📋 Statistical results summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d15f599f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-----------------+-----------------+------------------+--------------------+--------------------+\n",
      "|summary|             Price|             Open|             High|               Low|                 Vol|              Change|\n",
      "+-------+------------------+-----------------+-----------------+------------------+--------------------+--------------------+\n",
      "|  count|              2581|             2581|             2581|              2581|                2581|                2581|\n",
      "|   mean| 987.2235761332812|987.1161139093396|993.4640759395588|  980.269651297945|2.5384480379697792E7|3.930647036032540...|\n",
      "| stddev|261.97210258799754|262.1617610204898| 263.935589281456|259.84101849529037|1.5117566805804768E8|0.011492879930301337|\n",
      "|    min|            521.88|           511.13|           525.83|            511.13|             61680.0|             -0.0668|\n",
      "|    25%|            767.49|            767.3|           769.99|            762.53|            159910.0|             -0.0044|\n",
      "|    50%|            988.13|           987.89|            993.6|            983.35|            276730.0|              0.0011|\n",
      "|    75%|           1213.93|          1213.84|          1222.46|            1202.5|            694060.0|              0.0063|\n",
      "|    max|           1528.57|           1534.1|          1536.45|           1524.96|              1.99E9|              0.0677|\n",
      "+-------+------------------+-----------------+-----------------+------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Detailed summary statistics including percentiles\n",
    "vni_df.summary().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e27193",
   "metadata": {},
   "source": [
    "#### 📈 Key Inpretations:\n",
    "\n",
    "The table below presents descriptive statistics for the VN-Index (VNI) dataset, spanning **2,581 daily records** from 2015 to 2025. Here's a breakdown of key insights derived from the summary:\n",
    "\n",
    "- **Price Levels**:\n",
    "  - The **average closing price** of the VN-Index was approximately **987.22**, with values ranging from a minimum of **521.88** to a maximum of **1,528.57**.\n",
    "  - This reflects significant market growth over the decade and possible cyclical fluctuations.\n",
    "\n",
    "- **Volatility & Range**:\n",
    "  - The **standard deviation** of around **261.97** for `Price` and similar values across `Open`, `High`, and `Low` suggest moderate-to-high volatility in the VN-Index over time.\n",
    "  - The **interquartile range (IQR)** — from 25th percentile (~767.49) to 75th percentile (~1213.93) — further supports notable variation in price movements.\n",
    "\n",
    "- **Volume**:\n",
    "  - The `Vol.` column shows high dispersion in daily trading volume, with an average of **~25.38 million** shares per day and a maximum approaching **1.99 billion**.\n",
    "  - The large standard deviation (≈151M) indicates occasional surges in trading activity, likely due to macroeconomic events or investor sentiment shifts.\n",
    "\n",
    "- **Daily Change %**:\n",
    "  - The `Change` column reflects a **mean daily return** of approximately **0.0039 (or 0.39%)**, with a **maximum daily gain** of **6.77%** and a **maximum loss** of **-6.68%**.\n",
    "  - The relatively low median (0.11%) and standard deviation (~1.15%) point toward small daily fluctuations on most trading days, punctuated by occasional sharp moves.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916f90bb",
   "metadata": {},
   "source": [
    "#### 📅 Sorting by Date for Time Series Consistency\n",
    "When working with time series data such as VNI historical data, maintaining chronological order is essential. Without explicitly sorting the dataset, the rows may appear in a random or inconsistent order, which can lead to incorrect insights during trend analysis, forecasting, or visualizations. To ensure the dataset follows the correct timeline, we sort the data by the Date column in ascending order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7f546e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ The dataset has been sorted by date in ascending order.\n"
     ]
    }
   ],
   "source": [
    "# Sort by ascending date\n",
    "df_order = vni_df.orderBy(\"Date\")\n",
    "\n",
    "# Print confirmation message\n",
    "print(\"✅ The dataset has been sorted by date in ascending order.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5aa87b",
   "metadata": {},
   "source": [
    "### 3. Export Cleaned VNI Data to CSV and Stop Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e1dea2cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ The CSV file has been saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Save the result to a CSV file\n",
    "df_order.toPandas().to_csv(\"C:/Users/ADMIN88/Documents/Big data analysis/Final project/VNI_cleaned.csv\", index=False)\n",
    "\n",
    "# Print confirmation message\n",
    "print(\"✅ The CSV file has been saved successfully.\")\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78581a8",
   "metadata": {},
   "source": [
    "#### 💾 Saving the Cleaned Dataset to CSV\n",
    "\n",
    "After completing all necessary data cleaning and preprocessing steps, we export the final DataFrame to a CSV file for further analysis or sharing. The code performs the following:\n",
    "\n",
    "1. **Convert to Pandas DataFrame**:  \n",
    "   We convert the Spark DataFrame `df_order` to a Pandas DataFrame using `.toPandas()` for compatibility with the `.to_csv()` function.\n",
    "\n",
    "2. **Export to CSV**:  \n",
    "   The cleaned dataset is saved to the specified path\n",
    "\n",
    "3. **Terminate Spark Session**:  \n",
    "Finally, we stop the active Spark session with `spark.stop()` to release resources.\n",
    "\n",
    "This step finalizes our data pipeline and prepares the cleaned dataset for downstream tasks such as visualization, modeling, or reporting.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
